{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcce7126",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import pytorch_lightning as pl\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "from pprint import pprint\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75305ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from glob import glob\n",
    "\n",
    "img_fnames = sorted([f[21:] for f in glob('fashion_segmentation/png_images/IMAGES/*.png')])\n",
    "\n",
    "mask_fnames = [f'png_masks/MASKS/seg_{f[-8:-4]}.png' for f in img_fnames]\n",
    "for f in mask_fnames:\n",
    "    if not os.path.exists(os.path.join('fashion_segmentation', f)):\n",
    "        print(f)\n",
    "\n",
    "fnames = pd.DataFrame(data={'img': img_fnames, 'mask': mask_fnames})\n",
    "\n",
    "seed = 1337\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, val = train_test_split(fnames, test_size=0.1)\n",
    "\n",
    "train.to_csv('fashion_segmentation/train.csv', header=None, index=None)\n",
    "val.to_csv('fashion_segmentation/val.csv', header=None, index=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf71ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "\n",
    "class FashionSegmentationDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, mode=\"train\", transform=None):\n",
    "        assert mode in {\"train\", \"val\"}\n",
    "        self.root = root\n",
    "        self.mode = mode\n",
    "        self.transform = transform\n",
    "        self.filenames = self._read_split()  # read train/val split\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_filename, mask_filename = self.filenames[idx]\n",
    "        image = cv2.imread(os.path.join(self.root, image_filename))\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        mask = cv2.imread(os.path.join(self.root, mask_filename), cv2.IMREAD_UNCHANGED)\n",
    "        # mask = preprocess_mask(mask)\n",
    "        if self.transform is not None:\n",
    "            transformed = self.transform(image=image, mask=mask)\n",
    "            image = transformed[\"image\"]\n",
    "            mask = transformed[\"mask\"]\n",
    "        return dict(image=image, mask=mask)\n",
    "\n",
    "    @staticmethod\n",
    "    def _preprocess_mask(mask):\n",
    "        mask = mask.astype(np.float32)\n",
    "        mask[mask == 2.0] = 0.0\n",
    "        mask[(mask == 1.0) | (mask == 3.0)] = 1.0\n",
    "        return mask\n",
    "\n",
    "    def _read_split(self):\n",
    "        split_filename = \"val.csv\" if self.mode == \"val\" else \"train.csv\"\n",
    "        split_path = os.path.join(self.root, split_filename)\n",
    "        with open(split_path) as f:\n",
    "            split_data = f.read().strip(\"\\n\").split(\"\\n\")\n",
    "        filenames = [tuple(x.split(\",\")) for x in split_data]\n",
    "        return filenames\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Dataset checklist:\n",
    "[x] H and W are divisible by 32\n",
    "[x] CHW axes order\n",
    "[x] no leaks"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "train_transform = A.Compose(\n",
    "    [\n",
    "        A.SmallestMaxSize(256),\n",
    "        A.RandomCrop(256, 256),\n",
    "        A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=15, p=0.5),\n",
    "        A.RGBShift(r_shift_limit=15, g_shift_limit=15, b_shift_limit=15, p=0.5),\n",
    "        A.RandomBrightnessContrast(p=0.5),\n",
    "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "        ToTensorV2(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "train_dataset = FashionSegmentationDataset('fashion_segmentation', mode='train', transform=train_transform)\n",
    "\n",
    "val_transform = A.Compose(\n",
    "    [\n",
    "        A.SmallestMaxSize(256),\n",
    "        A.CenterCrop(256, 256),\n",
    "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "        ToTensorV2(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "val_dataset = FashionSegmentationDataset('fashion_segmentation', mode='val', transform=val_transform)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def visualize_augmentations(dataset, idx=0, samples=5):\n",
    "    dataset = copy.deepcopy(dataset)\n",
    "    dataset.transform = A.Compose([t for t in dataset.transform if not isinstance(t, (A.Normalize, ToTensorV2))])\n",
    "    figure, ax = plt.subplots(nrows=samples, ncols=2, figsize=(10, 24))\n",
    "    for i in range(samples):\n",
    "        sample = dataset[idx]\n",
    "        ax[i, 0].imshow(sample[\"image\"])\n",
    "        ax[i, 1].imshow(sample[\"mask\"], interpolation=\"nearest\")\n",
    "        ax[i, 0].set_title(\"Augmented image\")\n",
    "        ax[i, 1].set_title(\"Augmented mask\")\n",
    "        ax[i, 0].set_axis_off()\n",
    "        ax[i, 1].set_axis_off()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# visualize_augmentations(val_dataset)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "assert set(train_dataset.filenames).isdisjoint(set(val_dataset.filenames))\n",
    "\n",
    "print(f\"Train size: {len(train_dataset)}\")\n",
    "print(f\"Val size: {len(val_dataset)}\")\n",
    "\n",
    "n_cpu = os.cpu_count()\n",
    "batch_size = 16\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=n_cpu)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=n_cpu)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class FashionModel(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, arch, encoder_name, in_channels, out_classes, **kwargs):\n",
    "        super().__init__()\n",
    "        self.model = smp.create_model(arch, encoder_name=encoder_name, in_channels=in_channels, classes=out_classes, **kwargs)\n",
    "        params = smp.encoders.get_preprocessing_params(encoder_name)\n",
    "        self.loss_fn = smp.losses.DiceLoss(smp.losses.MULTICLASS_MODE, from_logits=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "\n",
    "    def common_step(self, batch, stage):\n",
    "        image = batch[\"image\"]\n",
    "        assert image.ndim == 4 # Shape of the image should be (batch_size, num_channels, height, width)\n",
    "        assert image.shape[2] % 32 == 0 and image.shape[3] % 32 == 0 # Check that image dimensions are divisible by 32 to comply with network's downscaling factor\n",
    "\n",
    "        mask = batch[\"mask\"].long()\n",
    "        assert mask.ndim == 3 # Shape of the mask should be [batch_size, num_classes, height, width]\n",
    "\n",
    "        logits_mask = self.forward(image)\n",
    "        loss = self.loss_fn(logits_mask, mask) # Predicted mask contains logits, and loss_fn param `from_logits` is set to True\n",
    "\n",
    "        prob_mask = logits_mask.sigmoid() # convert mask values to probabilities\n",
    "        pred_mask = (prob_mask > 0.5).float() # apply thresholding\n",
    "\n",
    "        tp, fp, fn, tn = smp.metrics.get_stats(torch.argmax(pred_mask, dim=1).long(), mask.long(), mode=\"multiclass\", num_classes=59)\n",
    "        return {\"loss\": loss, \"tp\": tp, \"fp\": fp, \"fn\": fn, \"tn\": tn}\n",
    "\n",
    "    def common_epoch_end(self, outputs, stage):\n",
    "        tp = torch.cat([x[\"tp\"] for x in outputs])\n",
    "        fp = torch.cat([x[\"fp\"] for x in outputs])\n",
    "        fn = torch.cat([x[\"fn\"] for x in outputs])\n",
    "        tn = torch.cat([x[\"tn\"] for x in outputs])\n",
    "\n",
    "        per_image_iou = smp.metrics.iou_score(tp, fp, fn, tn, reduction=\"micro-imagewise\") # calculate IoU for each image and then compute mean over these scores\n",
    "        dataset_iou = smp.metrics.iou_score(tp, fp, fn, tn, reduction=\"micro\") # aggregate intersection and union over whole dataset and then compute IoU score\n",
    "\n",
    "        metrics = {f\"{stage}_per_image_iou\": per_image_iou,\n",
    "                   f\"{stage}_dataset_iou\": dataset_iou}\n",
    "        self.log_dict(metrics, prog_bar=True)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self.common_step(batch, \"train\")\n",
    "\n",
    "    def training_epoch_end(self, outputs):\n",
    "        return self.common_epoch_end(outputs, \"train\")\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        return self.common_step(batch, \"valid\")\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        return self.common_epoch_end(outputs, \"valid\")\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=0.0001)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The difference between dataset_iou and per_image_iou scores in this particular case will not be much, however for dataset with \"empty\" images (images without target class) a large gap could be observed. Empty images influence a lot on per_image_iou and much less on dataset_iou."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = FashionModel(\"FPN\", \"resnet34\", in_channels=3, out_classes=59)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(\n",
    "    gpus=1,\n",
    "    max_epochs=10,\n",
    ")\n",
    "\n",
    "trainer.fit(\n",
    "    model,\n",
    "    train_dataloaders=train_dataloader,\n",
    "    val_dataloaders=val_dataloader,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# run validation dataset\n",
    "valid_metrics = trainer.validate(model, dataloaders=val_dataloader, verbose=False)\n",
    "pprint(valid_metrics)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "batch = next(iter(val_dataloader))\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    logits = model(batch[\"image\"])\n",
    "pr_masks = torch.argmax(logits.sigmoid(), dim=1)\n",
    "\n",
    "figure, ax = plt.subplots(nrows=batch_size, ncols=3, figsize=(10, 24))\n",
    "for i, (image, gt_mask, pr_mask) in enumerate(zip(batch[\"image\"], batch[\"mask\"], pr_masks)):\n",
    "    ax[i, 0].imshow(image.numpy().transpose(1, 2, 0))\n",
    "    ax[i, 1].imshow(gt_mask.numpy(), interpolation=\"nearest\")\n",
    "    ax[i, 2].imshow(pr_mask.numpy(), interpolation=\"nearest\")\n",
    "    ax[i, 0].set_title(\"image\")\n",
    "    ax[i, 1].set_title(\"gt mask\")\n",
    "    ax[i, 2].set_title(\"pr mask\")\n",
    "    ax[i, 0].set_axis_off()\n",
    "    ax[i, 1].set_axis_off()\n",
    "    ax[i, 2].set_axis_off()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
